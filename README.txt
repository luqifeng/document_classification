机器学习（进阶）纳米学位
文档归类 
吕奇峰

项目报告
项目背景
自然语言处理（NLP）【1】是人工智能极为重要的一部分。
是计算机科学，人工智能，语言学关注计算机和人类（自然）语言之间的相互作用的领域。因此，自然语言处理是与人机交互的领域有关的。在自然语言处理面临很多挑战，包括自然语言理解，因此，自然语言处理涉及人机交互的智能。在NLP诸多挑战涉及自然语言理解，即计算机源于人为或自然语言输入的意思，和其他涉及到自然语言生成。
	机器学习下的自然语言处理不同于一般的语言处理算法。在数据量不足，计算机算力也不大的时候，人们普遍大规模的使用规则模型去理解语意。将注意力主要集中在规则上，不同的规则将会输出不同的结果。而机器学习则改变着这种形态。机器学习通过海量的数据，淡化人为的规则，将更多的注意力放在了数据本身。通过巨大的算力，进行巨量的运算而自然生成自然语言的模型。这种做法为自然语言处理带来了跨越性的进展，突破了传统的瓶颈。
一些最早使用的算法，如决策树，产生硬的if-then规则类似于手写的规则，是再普通的系统体系。然而，越来越多的研究集中于统计模型，这使得基于附加实数值的权重，每个输入要素柔软，概率的决策。此类模型具有能够表达许多不同的可能的答案，而不是只有一个相对的确定性，产生更可靠的结果时，这种模型被包括作为较大系统的一个组成部分的优点。
自然语言处理研究逐渐从词汇语义进一步的到叙事的理解。然而人类水平的自然语言处理，是一个人工智能的极限问题。它是相当让人工智能和人一样聪明。这是自然语言处理的未来，因此密切结合人工智能发展。
问题描述
自然语言处理有个非常复杂的问题解决过程。首先是一个难点是分词，只是分词现在已经有非常多的库可以使用，也已经有相对比较成熟的算法，得到比较好的成功率。分词对于不同语言的含义是不一样的，比如中文，正向思维是把一个句子分成短语，短语分成词。而逆向思维则是字组成词，词组成短语，短语组成句子。而英文则不太一样，因为的最小单位是字母，字母组成词，词组成习惯短语，然后再组成句子。
分词之后就是词、语句以及文章的表达。以英文为例，最常见的词语表述方式比如”cat“、”dog“，这些都是利用字母表示意思。统计语言处理里面，比较容易利用字母来描述概率模型，比如ngram模型 ，计算两个单词或者多个单词同时出现的概率，但是这些符号难以直接表示词与词之间的关联，也难以直接作为机器学习模型输入向量。对句子或者文章的表示，可以采用词袋子模型，即将段落或文章表示成一组单词，例如两个句子：”She loves cats.“、”He loves cats too.“ 我们可以构建一个词频字典：{"She": 1, "He": 1, "loves": 2 "cats": 2, "too": 1}。根据这个字典, 我们能将上述两句话重新表达为下述两个向量: [1, 0, 1, 1, 0]和[0, 1, 1, 1, 1]，每1维代表对应单词的频率。
近年来，借助深度学习概念和性能强劲的硬件平台，Geofrey Hinton, Tomas Mikolov, Richard Socher等学者深入开展了针对词向量的研究，将自然语言处理推向了新的高度。以词向量为基础，可以方便引入机器学习模型对文本进行分类、情感分析、预测、自动翻译等。最简单的词向量就是独热编码(one-hot encoder)，比如有三个单词“man"、”husband“、”dog“，将之分别表示为[0,0,1]，[0,1,0]，[1,0,0]，这些词向量可以作为机器学习模型的输入数值向量，但是它们依然难以表达关联性，而且当词库单词量庞大时，独热编码的维度也会十分巨大，给计算和存储带来不少问题。Mikolov、Socher等人提出了Word2Vec、GloVec等词向量模型，能够比较好的解决这个问题，即用维数较少的向量表达词以及词之间的关联性。关于这些词向量模型的具体原理，可以阅读他们所发表的论文，主要是英文，中文网站上也出现了不少精彩的翻译和解读，可以参考某些关于自然语言处理的中文博客。
本项目目的就是利用上述自然语言处理技术结合所学机器学习知识对文档进行准确分类。
分析数据
分类文本数据使用经典的20类新闻包，里面大约有20000条新闻，比较均衡地分成了20类，是比较常用的文本数据之一。每个类的数量如下图所示，该图代码在Number_Plot.py：
 
数据以txt文件保存，除了正文，还有Header，Footer等各种附加信息。正文的风格，字数都有很大差异。随机选取一节加以分析：
Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!cs.utexas.edu!uunet!news.tek.com!vice!bobbe
From: bobbe@vice.ICO.TEK.COM (Robert Beauchaine)
Newsgroups: alt.atheism
Subject: Re: Amusing atheists and agnostics
Message-ID: <11860@vice.ICO.TEK.COM>
Date: 20 Apr 93 15:37:10 GMT
References: <timmbake.735204406@mcl> <madhausC5rFqo.9qL@netcom.com>
Organization: Tektronix Inc., Beaverton, Or.
Lines: 18

In article <madhausC5rFqo.9qL@netcom.com> madhaus@netcom.com (Maddi Hausmann) writes:
>
>"Clam" Bake Timmons = Bill "Shit Stirrer Connor"
>

  Sorry, gotta disagree with you on this one Maddi (not the
  resemblence to Bill.  The nickname).

  I prefer "Half" Bake'd Timmons

/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\ 

Bob Beauchaine bobbe@vice.ICO.TEK.COM 

They said that Queens could stay, they blew the Bronx away,
and sank Manhattan out at sea.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
 其文本内容与E-mail类似。有一系列头标签，比如Path,From,Newsgroups,Subject,Message-ID,Date,References,Organization,还有行数Lines. 文本内容短的只有10行左右，而长的则有上千行。
解决方法
解决问题首先需要了解可以使用的工具。首先来看CountVectorizer。这是一个将句子分成单词，然后变成矢量。在CountVectorizer.py中，我对其进行了简单的参数测试。
ngram_range，这个参数非常有意思，它可以让CountVectorizer不仅仅整合单个单词做特征，同时可以让两个以上的单词连起来做特征，比如ngram_range=（1，2）就会让特征包含两个连续的词。但这个参数虽然增加了特征，但会让计算量增多，并且过多的连续单词，也并不会增加整个文档的识别率。
stop_words，有很多词汇其实并没有领域内的意思，比如the，a，of等，这种词汇加入到词向量中，只会增加计算量以及误判的可能，所以将一些常见词抛弃是很有必要的，而SKlearn已经在库中集成了这部分，只需要将stop_words=’english’ 就可以屏蔽一些停止词。
另外还有token_pattern, 在运行各种测试的时候，我发现CountVectorizer经常会把数字也作为特征加进词向量中。但我并不认为数字作为分类的标准是一件很好的事情。我希望可以把数字部分的词向量全部删除。而CountVectorizer并没有特别原生的方法支持我这个思路。但我发现了token_pattern，这个选取词的过程，参数是正则表达式，例如使用'(?u)\\b[a-zA-Z]{2,10}\\b' 作为pattern时，就可以将全部的数字，或者含数字单词都删掉。同时还可以规定单词的字母数，比如我觉得当字母超过10个，就可能是乱打的词汇，进行屏蔽。下图为不同参数后的词向量结果：
 
另一个工具是TfidfTransformer，它可以将上面的词向量进行TF-IDF运算，得出权值。
利用Word2Vec【2】方式即词向量模型表示每篇文档，利用文本数据对词向量进行训练，将每个单词表示成向量形式。 首先用Word2Vec_CreateModel.py建立Word2Vec的模型，然后在Word2Vec_OneWordPic.py汇出某一个单词的相关词汇。如下图：
 
然后我尝试将整个Word2Vec的全部点都展示在一张图上，Word2Vec_BigPicture.py，如下：
  
 
由不同的参数，Word2Vec的整体图像差异度挺大，相对而言，我觉得词向量以及TF-IDF是一种比较稳定的算法。而Word2Vec，则变动幅度较大，有一定的随机性。针对文档分类而言，Word2Vec，并不是一种非常有效的方法。所以，最终还是以词向量和TF-IDF的结合，作为特征的基础。
基准模型
首先对比一下各种模型大致的区别，ALL_MODEL.py：
 
由图中可知，大部分的算法的分数基本一致，但训练的时间有较明显的差异，而测试时间则对于K邻近算法而言最为严重。选择MultinomialNB、LinearSVC、以及SGDClassifier，作为进一步了解使用的模型。
评估指标
本项目使用准确率作为评估指标。虽然二十分类这种多分类的问题可以用混淆矩阵来做细化的评估指标，但是我个人还是喜欢以一个分数来评价整个模型。所以可以将多分类，通过是非判断，简化为二分类的问题。从而建立二分类的评估标准和指标。公式如下：
准确率=(∑_(i=1)^n?〖A(y_i=Y_i)〗)/n
其中yi表示预测分类，Yi表示实际分类。而A函数表示如果两者相等，A=1，否则A=0 。
.
结论
首先，通过MultinomialNB_Search.py找一下较优的参数，如下：
 
然后通过SGDClassifier_Search.py 找一下较优的参数，结果如下：
 
最后是LinearSVC_Search.py 结果如下：
 
经过逐个测试，最后使用Final_Run.py得出较高的准确率，91%。

可视化图表
	首先做一个测试样本和准确率的关系图。以20为样本倍数，初始值设定样本数以及准确率皆为0，随着样本数的增多，准确率也稳步上升，到达一定程度后上升变得缓慢，逐渐稳定。
	代码为：Sample_Number_Plot.py
 
	
思考与改进
该项目问题的定义非常清晰，就是对文本数据的分类。
而分析的过程，就是对数据本身，包括格式，内容，特征等各种因素进行研究。同时也需要分析算法模型如何与实际数据结合，进而得出更好的结果。
文本处理的预处理同样包含很多内容。对于文本的标签行我觉得可以全部忽略。并且对全部的标点符号，特殊符号都需要去除。停止词同样全部筛选掉。min_df是个很有作用的参数，在没有设置这个参数时，可以看到关键词很多都是一些看似乱码的字符串，但是当min_df定义为足够的数值后，可以明显看出关键字的定义大部分可以理解，虽然正确率甚至有所降低，但对于语言的理解是更进一步的。
进而做最后选择的方法的描述。对于TF-IDF实际是以某词在某文出现次数占全文字数的百分比，以及含某词的文数占总文数的百分比的乘积来表示。而Word2Vec实际是对于相似词的聚合。我计划通过这两种模型的结合，得出某相似意义的词在某文全文字数百分比，和含某相似意义的词占总文书百分比的乘积来进行优化算法。但实际上，Word2Vec很难用于文本分类，它将近似意思的词汇汇总，但是得出的结论基本无法用于模型，所以最终还是以Tf-idf为主要的特征值
而具体的分类算法我觉得SVM会有比较高的分数，但最终是MultinomialNB有更好的准确率，可能与模型本身的参数设置有关。
因为该项目在网上并没有找到太多可以参考的分数，所以我的目标是让自己的正确率达到85%。但最终发现，在全部20类的文档下，正确率极难提高，所以减少分类至5类，比较轻松的获得了90%以上的准确率。
以上是我对这个项目的报告。
谢谢。

参考
	NLP：https://baike.baidu.com/item/nlp/25220
	word2vec：https://baike.baidu.com/item/Word2vec/22660840?fr=aladdin
	SVM： https://baike.baidu.com/item/svm 
